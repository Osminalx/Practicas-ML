version()
instal.packages("lenguageserver")
install.packages("lenguageserver")
q()
q()
install.packages("Ecdat")
install.packages("RCurl")
#cargan las librerias de clusters
# para visualizar resultados de análisis de clusters
library(factoextra)
data<-USArrests
#eliminar registros con valores ausentes
data<-na.omit(data)
#Identificar los datos
head(data)
ue las variables tengan el mismo peso en el análisis de
clustering.
data<- scale.default(data)
data<- scale.default(data)
#get_dist: fpara calcular una matriz de distancia entre las filas de una matriz de datos.
distance <- get_dist(data)
#La medida de distancia predeterminada es la distancia euclidiana
d <- dist(data, method = "euclidean")
# Agrupación jerárquica mediante vinculación completa
hc1 <- hclust(d, method = "complete" )
# Visualizar el dendrograma obtenido.
plot(hc1, cex = 0.6, hang = -1)
# Calcular con Agnes
hc2 <- agnes(data, method = "complete")
d <- dist(data, method = "euclidean")
# Agrupación jerárquica mediante vinculación completa
hc1 <- hclust(d, method = "complete" )
# Visualizar el dendrograma obtenido.
plot(hc1, cex = 0.6, hang = -1)
# Calcular con Agnes
hc2 <- agnes(data, method = "complete")
#análisis de agrupamiento jerárquico: average, single, complete y ward.
m <- c("average", "single", "complete", "ward")
hc2$ac
hc2 <- agnes(data, method = "complete")
m <- c("average", "single", "complete", "ward")
names(m)<-c("average","single", "complete", "ward" )
#función para calcular el coeficiente aglomerativo (agnes() leyendo el valor de $ac)
#calcula el coeficiente aglomerativo para cada método de vinculación.
ac <- function(x){ agnes(data,method = x)$ac}
#calcular el coeficiente de aglomeración para cada método de vinculación de
agrupaciones
#cargan las librerias de clusters
# para visualizar resultados de análisis de clusters
library(factoextra)
setwd("~/")
setwd("~/Documentos/5to semestre/Machine learning/Practica 15...creo")
library(gridExtra)
#Set de Datos
data("USArrests")
head(USArrests,10)
#calcular la varianza de cada variable
apply(USArrests,2,var)
#Crear un nuevo marco de datos con variables centradas
scaled_df <- apply(USArrests,2,scale)
head(scaled_df)
#Calcular valores propios y vectores propios
arrests.cov <-cov(scaled_df)
arrests.eigen <- eigen(arrests.cov)
str(arrests.eigen)
library(gridExtra)
#Set de Datos
data("USArrests")
head(USArrests,10)
#calcular la varianza de cada variable
apply(USArrests,2,var)
#Crear un nuevo marco de datos con variables centradas
scaled_df <- apply(USArrests,2,scale)
head(scaled_df)
#Calcular valores propios y vectores propios
arrests.cov <-cov(scaled_df)
arrests.eigen <- eigen(arrests.cov)
str(arrests.eigen)
#Tomar los primeros conjuntos
(phi <- arrests.eigen$vectors[,1:2])
phi<-phi
#El conjunto de cargas para el primer componente
row.names(phi)<- c("murder","Assault","UbanPop","Rape")
colnames(phi)<- c("PC1","PC2")
phi
#Calcular puntuaciones
PC1 <- as.matrix(scaled_df) %%phi[,1]
PC2 <- as.matrix(scaled_df) %%phi[,2]
#Crear marco de datos con puntuaciones de componentes
PC <- data.frame(State= row.names(USArrests),PC1,PC2)
head(PC)
#Plot
ggplot(PC,aes(PC1,PC2))+
modelr::geom_ref_line(h=0)+
modelr::geom_ref_line(v=0)+
geom_text(aes(label = State), size=3)+
xlab("Primer componente principal")+
ylab("Segundo componente Principal")+
ggtitle("Primeros dos componentes primcipales de USArrests Date")
#librerias
library(tidyverse)
